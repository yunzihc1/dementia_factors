# dementia_factors
## Objective
This study aims to explore previous risk factors in dementia using a public national dataset. Leveraging various machine learning algorithms, the researcher seeks to discern the relative importance of variables contributing to dementia and, ultimately, predict an individual's likelihood of developing this condition. Through this research, we aim to provide valuable insights that inform individuals about the choices they can make to safeguard their cognitive health.

## Methods
### Data Sources

The investigation utilized data derived from the Behavioral Risk Factor Surveillance System (BRFSS), a nationally representative survey designed for U.S. residents. The surveys encompassed inquiries about health-related risk behaviors, chronic health conditions, and utilization of preventive services. The dataset under consideration spans 2015 to 2022, with an annual compilation of over 400,000 interviews (BRFSS Archived, 2022). While the BRFSS survey's inception dates back to 1984, the specific question related to dementia was introduced in 2015. Notably, the BRFSS dataset is publicly accessible, facilitating direct download by researchers from official websites.
The dependent variable in this study was extracted from the query, "What is the main health problem, long-term illness, or disability that the person you care for has?" A caregiver reported this information, and the response options included "Dementia or other Cognitive Impairment Disorder," serving as the dependent variable denoting dementia. Other health problems were categorized as non-dementia. The independent variables in the study encompassed various risk factors, such as obesity, physical activity, smoking, alcohol use, stroke, depression, kidney disease, diabetes, and heart disease. Obesity was dichotomously recorded, with 1 indicating the presence of obesity and 0 denoting its absence.
Additionally, demographic variables were considered independent factors, including age, sex, race, employment status, education level, income, marital status, and healthcare access. For the race variable, six options were provided, with four races analyzed due to limited representation in other categories: White, Black, American Indian or Alaska Native, and Asian. The healthcare access variable measured whether respondents identified one person or a group of doctors as their designated personal healthcare provider.


### Data Preparation

Before modeling, the dataset underwent meticulous preparation involving feature selection, addressing missing values, mitigating class imbalance, and executing feature transformations. This preparatory phase was executed using Python and R programming languages, with the latter utilized for transferring the SAS dataset to CSV files. Key libraries and packages, notably scikit-learn, were employed in the data analysis.
Optimal feature selection was accomplished by leveraging both the Lancet and Libra index features (Bin-Hezam & Ward, 2019) and discerning relevant features within the context of the available BRFSS dataset.
Addressing missing values constituted a pivotal aspect of the data preprocessing phase, especially given the numerous missing entries in the outcome variables. The chosen strategy involved excluding data with missing values. For other missing data within the independent variables, the SimpleImputer method was employed to replace missing values with the most frequently occurring data during the feature transformation process.
To tackle imbalanced class distributions, a combination of methods was implemented, including SMOTE-Tomek Links. SMOTE generated synthetic samples for the minority class based on the feature space, while Tomek Links removed data identified as Tomek links from the majority class (Viadinugroho, 2021). A set of ratios was established at 1:5 for minority and majority, thereby enhancing the robustness of the classification model.
Following these preprocessing steps, the dataset underwent a stratified split, dividing it into a 70% training set and a 30% testing set. This partitioning ensured a representative distribution of classes in both subsets. The classification models chosen for this study prioritized interpretability. Logistic regression, random forest, decision tree, and gradient-boosting machine classifiers were deliberately selected for their ability to provide a transparent understanding of the underlying relationships within the data.

### Evaluation
Post-training, the models' results were subjected to evaluation to identify potential issues with overfitting and parameter tuning. Various metrics were implemented in this study, including accuracy, precision, recall, F1-score, confusion matrix, and receiver operating characteristic (ROC). Additionally, this study utilized two years of data to validate the final model.

